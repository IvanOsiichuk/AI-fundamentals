{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# The use of neural networks in semantic search (BERT, GPT)",
   "id": "b34ba60506aff1e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ML in NLP\n",
    "\n",
    "### Embedding\n",
    "\n",
    "- Embedding in machine learning refers to a representation learning technique that maps complex, high-dimensional data into a lower-dimensional vector space of numerical vectors.\n",
    "\n",
    "#### Word embedding\n",
    "\n",
    "- In natural language processing (NLP), words or concepts may be represented as numerical feature vectors, where similar concepts are mapped to nearby vectors.\n",
    "- The primary goal of word embeddings is to represent words in a way that captures their semantic relationships and contextual information.\n",
    "- Typically, the word representation is a real-valued vector that encodes the meaning of the word in such a way that the words that are closer in the vector space are expected to be similar in meaning.\n",
    "    - These vectors are numerical representations in a continuous vector space, where the relative positions of vectors reflect the semantic similarities and relationships between words - distance and direction between vectors encode the degree of similarity.\n",
    "- The reason vectors are used to represent words is that most machine learning algorithms, including neural networks, are incapable of processing plain text in its raw form, requiring numbers as input.\n",
    "- The process of creating word embeddings involves training a model on a large corpus of text.\n",
    "    - The corpus is preprocessed by tokenizing the text into words, removing stop words and punctuation and performing other text-cleaning tasks.\n",
    "    - A sliding context window is applied to the text, and for each target word, the surrounding words within the window are considered as context words.\n",
    "\n",
    "### Example\n",
    "\n",
    "A simplified example of word embeddings for a very small corpus (6 words), where each word is represented as a 3-dimensional vector:\n",
    "```\n",
    " cat    [0.2, -0.4, 0.7]\n",
    " dog    [0.6, 0.1, 0.5]\n",
    " apple  [0.8, -0.2, -0.3]\n",
    " orange [0.7, -0.1, -0.6]\n",
    " happy  [-0.5, 0.9, 0.2]\n",
    " sad    [0.4, -0.7, -0.5]\n",
    "```\n",
    "- For instance, the vectors for \"cat\" and \"dog\" are close together, reflecting their semantic relationship.\n",
    "- Likewise, the vectors for \"happy\" and \"sad\" have opposite directions, indicating their contrasting meanings.\n",
    "\n",
    "- Each value in an embedding vector is a specific dimension representing a specific characteristic of each word (or sentence for corresponding embeddings) within that dimension. For simplified understanding:\n",
    "    - Dimension 1 might partly reflect something about gender (e.g., “king” vs. “queen”).\n",
    "    - Dimension 2 might encode tense or morphological form.\n",
    "    - Dimension 3 might reflect semantic domain (e.g., animals vs. tools).\n",
    "- But in reality, the vectors rather encode semantic and syntactic features learned from patterns in the data.\n",
    "- Individual words or sentences can be represented as points within an n-dimensional vector space.\n",
    "    - Units that are close to each other in this space have similar values within their dimensions, and therefore similar values for the model.\n",
    "\n",
    "    ![vector_space.png](https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SemanticSearch.png)\n",
    "\n",
    "### ML on the basis of word embeddings\n",
    "\n",
    "There are two popular approaches to learning word embeddings:\n",
    "1. The Word2Vec model, introduced by Tomas Mikolov and his colleagues at Google in 2013, marked a significant breakthrough.\n",
    "    - Word2Vec leverages two models, Continuous Bag of Words (CBOW) and Continuous Skip-gram.\n",
    "    - There techniques efficiently learn word embeddings from large corpora and have become widely adopted due to their simplicity and effectiveness.\n",
    "2. GloVe (Global Vectors for Word Representation), introduced by Pennington et al. in 2014.\n",
    "    - GloVe is based on the idea of using global statistics (word co-occurrence frequencies) to learn vector representations for words.\n",
    "    - The method has been used in various NLP applications and is known for its ability to capture semantic relationships.\n",
    "\n",
    "## References\n",
    "- https://en.wikipedia.org/wiki/Embedding_(machine_learning)\n",
    "- https://en.wikipedia.org/wiki/Word_embedding\n",
    "- https://www.ibm.com/think/topics/word-embeddings"
   ],
   "id": "b661fdc3b3b550c3",
   "attachments": {
    "d2c46710-ad3e-4cb6-ad99-e43e7bf49f2f.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAAFZCAIAAACqsltFAAAW7klEQVR4Xu3d/49lZX3A8fsjP+78B5hNoUmbAukQfyC0tiEZo/EHjXE2NoFtE1PH2q6A1TpuXa2xOFS6ll3opG1cROpKbGhcM1QiKmFchKVLFmQXLA5bzHaL3ZURcMMX295+uJ/OwzPP+dw7Z+49557ny/uVyWTm3HPvDkze9zn3uc850+sDKE8v3ACgAJQPlIjygRJRPlAiygdKRPmY1NLSUq/XO3HiRHgDIkb5RTt8+HBvs/n5+ZWVlXC/kbT81dXV8AZEjPKLptHOzc0tDSwsLGj/8owQ7jpcbOUvLy9H9fPEifKLptHKZ7dFgpEtO3fu9PbaQmzlx/bzxInyi1YtX+iw728ZLbbSYvt54rSNXzDyUy3//PnzsmVmZsbbq7+2tiYvBGSj3iRfy27u1mppI/afnZ2VLf7d+5sfQe8rBx36BDQ/P+/PHeohiXyWQ3rdRz67n1/uq/fyMfVoovyiBeVLJFKmv0U3zgwsLi7KdklRdpDd3A5B+aP3152lW3d3IfW65xq9r8S/tDHv4D9TaPn6Q+o+M4PnFzcxIVvm5ubcrSJ4loGi/KJphz6JMMhSQpK6/JFTp9D82Hpe+aP312HZf+KQPXuDUN3O7iYhTx9yq3u7Qcv3H1/vPuKZCCbKL5pGonP7boD1o9WDf+lq1aMlS5P+g2hpdfbXMVmeAvRb/XeHHZOvDlJf2jgGCb5VegjgvqX8Oii/aBpJ0JU/sa9bTHIY7z+IllZnf11E4P5Rea4J3kqQHeRpYm4gePVhlq9PJe5byq+D8osWlN/fGIGDo+tgDFdu0K6WP3p/OS5wtcs/1PNe9stNmnqA8htH+UWrlq8puvFZX5ZLWm6HKr+0Ovv3vSN8/cI9KejrArm7O/gPUqf8plB+0arl9wcz7X6N+m0QkjxBmGN+v8b++m1v8MpfBn/3LNPfeCh/+XDwE9YpX58+gplCBCi/aGb5Op0exNbz3ifTA3I3LAflb7m/cu/Y+yuF9b7uHUF9879X+WGCHzgoX2f75Y5Lg2lL/xkHDuUXzRweq0fs0pKMzBqhjtL+kK7l+2GP3l/pvWY2LxnqDw4H3Et9uaP+MO4n1PLN9x39LUsb7/PL8wvv55soHygR5QMlonygRJQPlIjygRJRPlAiygdKRPlAiSgfKBHlAyWifKBElA+UiPKLUP1bOrOzs0tcnbJglF8EPTFOr7cnFhcX9STZmZmZ7f4trXTpiYnVswbLRPlF0PKD09q1BIm/kDPY9X8C5SvKL4JZvtvurnidN8r3UX4RhpWvF8nubf5bWtKG/nkMPSIwL2sjD2X+GRy9Y3D5Hd3H/1buLv+KXoFDL57T3/jrOvqYC5v/jE9/8KMO+0dV9WHdg+jlPQLDrvNdCMovwrDy+xuXsnIjoc4Fuuthackzmy/Cr3fRWQO9lp67gE/waEp39r/VOPVyXRrzwuCvcemfynKP7+7irsmrd9Gdg9cp7mH1B3P7663uYd01wgqf3aT8ItQsX6+HHXSu0wFu0A4uztsf3MtNE9Ysv+ddfs8NyP6LDn06cHHqz+9fsU8vtuffJXhYua/5IBztK8ovQs3ydcB3fwzHmRlc006/1gPyYf3ULN//tr/Rub9FrwLqHkcH8NXNepv/qFb1YYMHoXwf5Rdhy/J1kNfdqu/z6T56aK1fBzs445Vffcyg0t4QM96FN3uVhw0ehPJ9Q3+FyMmw8nWGz/UTeflvDvcb/Fcl1Yel/BGG/gqRk2Hl62t492o5+NOXjr741691zm9YP+atk5dffTlQVX1Yyh9hi/+byINZvs7V+TPkOtk24/29+n7l2UHnAobN8AX/kNykzwUTlq+TC8HPLwO+n3H1YYMHMf+4QLEovwjawJy3ete96R28ra2zYjMb7+ppk7Kz/1zgv3mmC4Fdcv5E/dLgHTu3s7t7NdEty9c3Hfz/BH1C8TOuPmzwIPztHR/lF6F6xo5EItmY72nLzu6v3Ogb7MFu8q2+o96z/oTOamUtje7jduhtPmToD8r35+r6lWj7G/+oW8kjd/Hf5OtbD6sP4j+1LfG3dzZQPlAiygdKRPlAiSgfKBHlAyWifKBElA+UiPKBElE+UCLKB0pE+UCJhpZ/+vTpcBMwli9/67lwE7pml7++vn7xxReHW4Gx/MHSY8+evRBuRafs8vft29fr9eRzeAOwfZ8+9NQDJ86FW9Epo3wZ8Hfs2CHly+fwNmD7vnTvv99530/CreiUUb4O+IphH5O7//h/ffbLT4db0amwfDfgK4Z9TO6ZM7/4wF89Fm5Fp8Ly/QFfMexjQq/98n/mPno03IpObSo/GPAVwz4md+1f/utzzzO9H5FN5VcHfMWwjwnt/YdTq08Ufd272Gwqf//+/fsG/OaFbPd3A7br7755+h+/zfR+RMLX+cqVH94AjOW+R396010/CreiO3bblI9mPfXcywt/XfTfq4+N3Tblo1kXXv3vd378oXArumO3Tflo3O999tH/OPdKuBUdsdumfDRu8e9P/uDkz8Kt6IjdNuWjccvfOH33d8+EW9ERu23KR+Puffj5m7/6b+FWdMRum/LRuCefffHDX2R6PxZ225SPxr104fV3feIH4VZ0xG6b8tGGXZ859vwLr4Zb0QW7bcpHGz62/OSxp9bDreiC3Tblow0H71n7+gNM70fBbpvy0YYjR8/ecvcz4VZ0wW6b8tGGx3/84p4DT4Rb0QW7bcpHG9Zffv3dex8Ot6ILdtuUj5a891OPnPs50/vds9umfLTkxtt/ePxHRUzvr6yszM/Pz8zMaEqzs7PLy8vnz8dyYSK7bcpHS/7mn9b+efVsuDU7CwsLko9kL18sLS0tLi7u3LlT+48kfrttykdLJHuJP9yaF0ndjFyfDubm5vyNXbHbpny0RA71b7gt5+l9qb03GO3NsV1H/hMnuj9/wW6b8tGScz9/7b2feiTcmpHDhw9LOHJ4H94woIcD8lm/lacA+XZ+fr66j//ssLa2JscLOmWgryD8p5XV1VXZLp/lH9Ud5L76tfwwbje3p/7rdtuUj/a8e+/D6y+/Hm7NhZmco+25A/7gW6Xly036rWQ8MyCPLDfJ00Rv8FLC7a8PIlt0H3leODHQqzyn6MsNfU6x26Z8tGfPgSce//GL4dZcSMZ+t4ExypdbdRh3OywvL/e8Jxd9EJe0o68s3NGBfCGP454y7LY3wrdvBSZxy93PfPOh/wy35qLZ8nXWQHJd9Wj57gWFPkj19YXuJp/1W30Z4r6126Z8tOfrD5w5eE+20/t6RL2yshLeMKCVyj7+tyPK1x1M7khe93FzB87a2lrPe12gLxNko35rt+0ePbwBmNgjp1742PKT4dZcBANyQGcB3MBbs/xgzFeu4dUh5fe92vXYwX/Zb7dN+WjP8y+8uuszx8KtudCpNXmNHd4wOHTX194uWt15RPk6bo9eAjCifDn06A2ehvRQ3593tNumfLTh+Kkze2+97x0Ld1zxvtvks3wtW8KdIva/r7z02qnv/uKefRfu/UJ4m0df6rtDeiXZ6wuB4HCgt/nNf2lV373T8vsbE3XuW7dbnTFfyKPJI+g0ob/dbpvy0bj9d37/yl0Hr/r9u377j751zUcelM9vvfbOK+cPfvErR8NdI/P6s49e+M7fvnj7rvVb3vnyV2949bFvyFNAuJNHMpbj895g5Ne34tzq3V7lDT99OpBbZTf9Wu/rUtewe4OnkqUB3cHN5I8uX19f6N397XbbumuP8tEQyfvKXbf/7p7vXXPD9/0P2TK763Z5Ugjv0DVpW4KX4V1ql+ZlkP/l2adHB++T+OXFvCbaG4zq8hpbmtfxXEZgF7bsKXHqdtlfBnOdKfDfopOv3ck/+lD+IYCW7+YOAvp6IXjAPuVjCuSQXsb2avZvxj9/IJLDfsn7lYfu0uH9pS99QIZ62RLuNAHpUF8LjGi1Wfo2fnXewW6b8tGgP9v/xoF9tXn3IbfKPuHdpkWHdz2ef+FzvyXDu3xbf3gfgwz+kuJ0yh+2mthum/LRoHcs3PHGa/tK8O5Dbn37B+8I79YyaVtescvr9paG90joIYabDnTstikfDbrsPfuvuX61GvybHx95UPYJ79YCf3ivOV2XNH0b33xT0G6b8tGgd334zrf98bfD2r0PGfPluCC8W3Pcu3FyMP/GdN1geM84+DrstikfDdp7631vvfYr1eDdR0uv89uerkua3Tblo0HHT52ZnT8wnbn96U/XJcpum/LRrMEynqHv59986MHwDtskbcvwLgM7w3tNdtuUj8a9sZhn/qAc2Ptr+H7zfQduPmSf0LqlYHjPfrquWXbblB9wC84ve8/+FBecR0L+p330C/+i/xvf/sE75Ovq/8bzTx87vnzj/ddffc/73yKf5evzT206vYfpukbYbVO+L90F58k5efimI7t/9eiHfv30n/7GuU9eIZ+P/uGlR6679NTXPi95u3fj5Hheju05np+E3TblO8ktOE+X5L2y+9Kzn7jsZ39+hf8hWx7f82s//dzvMF3XILttylcJLThPnRzSy9hezd7Ff+S6S+SFQHg3jMtum/JV5AvOc3L8tj+RA/tq8+5DbpV9wrthXHbblK/iXHCepfuvv1pe1VeDdx9y6/17rgrvhnHZbVO+imfBefbuef9bzu8Na/c/zn3yCtknvBvGZbdN+arzBefl+M6Nb/vJx7ca86+/OrwbxmW3TfmqqwXnBTq+fCOv86fJbpvy1TQXnBfu/NPHjlx3yRZz+5uX9GASdtuU77S94BzOycM3DXs//97dl5y86y/CO2ACdtuU72t8wfk0pbXu+NTXPn/kukvlwH7TGr5rf4XsG2e3TfmBOgvOI5TiumM5pD9+64f+f93+nqvkaxbwtMFum/IzwLpjjGC3TfmpY90xRrPbpvzUse4Yo9ltU/6EOp9XY90xRrPbpvxJxDCvxrpjjGa3Tflji2RejXXHGM1um/LHE8+8GuuOMZrdNuWPJ555NdYdYzS7bcofT1Tzaqw7xgh225Q/ntjm1ZJed4xW2W1T/nginFdLdN0x2ma3TfnjYV4NqbDbpvzxMK+GVNhtU/7YmFdDEuy2KX8SzKshfnbblD+hpOfVOj/pAFNgt035xYrhpANMgd025ZcpkpMOMAV225RfoHhOOsAU2G1TfoHiOekAU2C3TfkFiuqkA7TNbpvyCxTbSQdold025RcowpMO0B67bcovECcdFMVum/ILxEkHRbHbpvwycdJBOey2Kb9YnHRQCLttyi9Z0icdoCa7bcoH8ma3TflA3uy2KR/Im9025QN5s9umfCBvdtuUD+TNbpvygbzZbVN+6riWHkaz26b8pHEtPWzJbpvy08W19FCH3TblJ4pr6aEmu23KTxTX0kNNdtuUnyiupYea7LYpP1FcSw812W1TfqK4lh5qstum/ERxLT3UZLdN+YniWnqoyW6b8tPFtfRQh9025SeNa+lhS3bbeZdfwpr2dK+lV8JvJwZ22xmXz5r2mPHbmRq77VzLZ017zPjtTJPddpbls6Y9Zvx2psxuO8vyWdMeM347U2a3nWX5rGmPGb+dKbPbzrJ81rTHjN/OlNltZ1k+a9pjxm9nyuy2syyfNe0x47czZXbbWZbPmvaY8duZMrvtLMvvs6Y9bvx2psluO9fy+6xpjxu/namx2864/H7Ka9pLwG9nOuy28y4fgN025QN5s9umfCBvdtuUD+TNbpvyA1wuApmx26Z8H5eLQH7stinf4XIRyJLdNuUrLheBXNltU77ichHIld025SsuF4Fc2W1HWH4ns+tcLgK5stuOrfyuZte5XARyZbcdVfkdzq5zuQjkym47nvK7nV3nchHIld12POV3PrvO5SKQJbvteMqPYXY96ctFdDIzivjZbcdTfiSz64leLqKrmVHEz247nvKZXR9bhzOjiJ/ddjzlM7s+nm5nRhE/u+14ymd2fTydz4wicnbb8ZTfZ3Z9LDHMjCJmdttRld9PfHa9E5HMjCJadtuxld9Pdna9K8yMYjS77QjLx7YwM4rR7LYpP3XMjGI0u23KzwAzoxjBbjuD8lm12mdmFMPZbadePqtWHWZGYbLbTrp8Vq2WhuO7Mdhtp1s+q1ZLw/HdeOy20y2fVatF4fhubHbb6ZbPqtVycHw3CbvtdMtn1Wo5OL6bhN12uuWzarUcHN9Nwm473fJZtVoOju8mYbedbvmsWi0Hx3eTsNtOt/w+q1aLwfHdJOy2ky6/z6rVMnB8Nwm77dTL77NqtQwc343NbjuD8lEIju/GY7dN+UgIx3djsNumfCBvdtuUD+TNbpvygbzZbVN+nDgRHU2x26b8CHEiOhpkt035seFEdDTLbpvyo8KJ6Gic3TblR4UT0dE4u23KjwonoqNxdtuUHxVOREfj7LYpPyqciI7G2W1TflQ4ER2Ns9um/KhwIjoaZ7dN+bHhRHQ0y26b8sfQ9tJaTkRHg+y2KX+7prO0lhPR0RS7bcrfFpbWIjl225RfH0trkSK7bcqvj6W1SJHdNuXXx9JapMhum/LrY2ktUmS3Tfn1sbQWKbLbpvz6WFqLFNltU359LK1Fiuy2KX9bWFqL5NhtU/52Jb20tu11x4iQ3TbljyHRpbXTWXeM2NhtU34hWHdcLLttyi8B645LZrdN+SVg3XHJ7LYpvwSsOy6Z3Tbll4B1xyWz26b8EqS17pi3Hptlt035JUho3TFvPTbObpvyS9DGuuM2RmbeemyD3TblF6LZdcdtjMy89dgSu+1Cym9jgEpOU+uOWxqZeeuxJXbbJZTfxgCVqMnXHbc3MvPWY0vstrMvv6UBqljtjcy89dgSu+28y29vgCpWeyNzWm89JsRuO+/y2xugitXeyJzQW49psdvOu/z2BqhitTcyt/HWI/pllt/eAFWsVkfmZt96hLLbzrv89gaoYrU9Mjf11iMcu+28y291gCpW2yPz5G89wme3nXf5bQ9QxWJkTojddt7l99sfoIrFyJwKu+3sy+8zQKFsdtsllN9ngELB7LYLKR95O84ZWcPZbVM+UscZWaPZbVM+ksYZWVuy26Z8pIszsuqw26Z8pIszsuqw26b8dDGtxRlZddhtU36imNbqc0ZWPXbblJ8iprUUZ2TVYbdN+clhWsvhjKw67LYpPzlMazmckVWH3TblJ4dpLR9nZG3Jbpvyk8O0VoAzskaz26b85DCtVcUZWSPYbVN+cpjWwrbYbVN+chqf1mJFUN7stik/RQ1Oa7EiKHt225SfqEamtVgRVAK7bcpP14TTWqwIKoTdNuUXixVBhbDbpvxisSKoEHbblF8sVgQVwm6b8ovFiqBC2G1TfrFYEVQIu23KL1bjK4IQJ7ttyi9ZgyuCEC27bcovXCMrghAzu23Kx4QrghA5u23KB/Jmt035QN7stikfyJvdNuUDebPbpnwgb3bblA/kzW6b8oG82W1TPpA3u23KB/Jmt035QN7stikfyJvdNuUDebPbpnwgb3bblA/kzW6b8oG82W1TPpA3u23KB/Jmt035QN7stikfyJvdNuUDebPbpnwgb3bblA/kzW6b8oG82W1TPpA3u23KB/Jmt035QN7stikfyJvdNuUDebPbpnwgb3bblA/kzW6b8oG82W1TPpA3u23KB/Jmt035QN7stikfyJvdNuUDebPbpnwgb3bblA+0bffu3YcOHQq3TovdNuUDbbv88sslMfncSf9225QPtE3LV9Pv327b/UAApmaa/dvl79ix46KLLgp/LgDtk9f/6+vrYZNNs8sX+/btC38iAG2SMX///v1TyL4/onwArQpe50+teUX5QDfc3P6Um1eUD3RDXs930ryifKBElA+U6P8AsaOJ/yvb+PkAAAAASUVORK5CYII="
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## SentenceTransformers (SBERT) library\n",
    "\n",
    "### Summary\n",
    "\n",
    "Sentence Transformers (a.k.a. SBERT) is the go-to Python module for accessing, using, and training state-of-the-art embedding and reranker models. It can be used to compute embeddings using Sentence Transformer models, to calculate similarity scores using Cross-Encoder (a.k.a. reranker) models, or to generate sparse embeddings using Sparse Encoder models (quickstart). This unlocks a wide range of applications, including semantic search, semantic textual similarity, and paraphrase mining.\n",
    "\n",
    "### Semantic search\n",
    "\n",
    "The idea behind semantic search is to embed all entries in your corpus, whether they be sentences, paragraphs, or documents, into a vector space. At search time, the query is embedded into the same vector space and the closest embeddings from your corpus are found.\n",
    "\n",
    "- For symmetric semantic search your query and the entries in your corpus are of about the same length and have the same amount of content. In this case, you could potentially flip the query and the entries in your corpus.\n",
    "- For asymmetric semantic search, you usually have a short query (like a question or some keywords) and you want to find a longer paragraph answering the query.\n",
    "\n",
    "### Re-ranking\n",
    "\n",
    "TODO\n",
    "\n",
    "### References\n",
    "- https://sbert.net"
   ],
   "id": "8f08652bfa95ad75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Transformer architecture\n",
    "\n",
    "### Summary\n",
    "\n",
    "The transformer is a neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.\n",
    "\n",
    "Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM)\n",
    "\n",
    "Each of the above parts is a separate component of the architecture transformer.\n",
    "\n",
    "### Attention mechanism\n",
    "\n",
    "An attention mechanism is a ML technique that directs deep learning models to prioritize (or attend to) the most relevant parts of input data. Mathematically speaking, an attention mechanism computes attention weights that reflect the relative importance of each part of an input sequence to the task at hand.\n",
    "\n",
    "1. A process of “reading” raw data sequences and converting them into vector embeddings, in which each element in the sequence is represented by its own feature vector(s).\n",
    "2. A process of accurately determining similarities, correlations and other dependencies (or lack thereof) between each vector, quantified as alignment scores (or attention scores) that reflect how aligned (or not aligned) they are.\n",
    "    - Alignment scores are then used to compute attention weights by using a softmax function, which normalizes all values to a range between 0–1 such that they all add up to a total of 1.\n",
    "    - So for instance, assigning an attention weight of 0 to an element means it should be ignored. An attention weight of 1 means that element should receive 100% attention because all other elements would have attention weights of 0 (because all weights must sum up to 1).\n",
    "    - In essence, the output of a softmax function is a probability distribution.\n",
    "3. A process of using those attention weights to emphasize or deemphasize the influence of specific input elements on how the model makes predictions. In other words, a means of using attention weights to help models focus on or ignore information.\n",
    "\n",
    "### Queries, keys and values\n",
    "\n",
    "The seminal “Attention is All You Need” paper articulated its attention mechanism by using the terminology of a relational database: queries, keys and values:\n",
    "\n",
    "- The query vector represents the information a given token is seeking.\n",
    "- The key vectors represent the information that each token contains. Alignment between query and key is used to compute attention weights.\n",
    "- The value (or value vector) applies the attention-weighted information from the key vectors. Contributions from keys that are strongly aligned with a query are weighted more heavily; contributions from keys that are not relevant to a query will be weighted closer to zero.\n",
    "\n",
    "Therefore, each vector is calculated as follows using embedding of each token:\n",
    "\n",
    "- The embedding is multiplied by the weight matrix $W_q$ to yield the query vector $Q$\n",
    "- The embedding is multiplied by the weight matrix $W_k$ to yield the key vector $K$\n",
    "- The embedding is multiplied by the weight matrix $W_v$ to yield the value vector $V$\n",
    "\n",
    "### Self-attention\n",
    "\n",
    "The earliest types of attention mechanisms all performed what is now categorized as cross-attention. In cross-attention, queries and keys come from different data sources. In self-attention, queries, keys and values are all drawn from the same source.\n",
    "\n",
    "Consider a language model interpreting the English text `on Friday, the judge issued a sentence`:\n",
    "\n",
    "- The preceding word `the` suggests that `judge` is acting as a noun as in, person presiding over a legal trial rather than a verb meaning to appraise or form an opinion.\n",
    "- That context for the word `judge` suggests that `sentence` probably refers to a legal penalty, rather than a grammatical “sentence.”\n",
    "- The word `issued` further implies that sentence is referring to the legal concept, not the grammatical concept.\n",
    "- Therefore, when interpreting the word `sentence`, the model should pay close attention to `judge` and `issued`. It should also pay some attention to the word `the`. It can more or less ignore the other words. A well-trained self-attention mechanism would compute attention weights accordingly.\n",
    "\n",
    "The attention mechanism’s primary function is to weight the importance of the query-key pairings between each token. For each token x in an input sequence, the transformer model computes (and then applies) attention weights as follows:\n",
    "\n",
    "1. Token x’s query vector $Q_x$ is multiplied by each other token’s key vector $K$.\n",
    "    - The resulting dot product will be large for a token that’s highly relevant; its dot product with an irrelevant token will be small or negative.\n",
    "2. Each dot product will be scaled - that is, multiplied—by $\\frac{1}{\\sqrt{d_k}}$.\n",
    "    - The result is the alignment score between token x and each other token.\n",
    "3. These alignment scores are input to a softmax function, which normalizes each score to a value between 0–1, such that they all add up to 1.\n",
    "    - These are the attention weights between token x and each other token.\n",
    "    - You can think of each token as now having a corresponding vector of attention weights, in which each element of that vector represents the extent to which some other token should influence it.\n",
    "4. Each other token’s value vector is now multiplied by its respective attention weight.\n",
    "5. These attention-weighted value vectors are all averaged together.\n",
    "    - The resulting vector represents the average of all the attention-weighted contributions from each key vector.\n",
    "6. Finally, the resulting vector of changes for each token is added to token x’s original vector embedding.\n",
    "    - In essence, token x’s vector embedding has been updated to better reflect the context provided by the other tokens in the sequence.\n",
    "\n",
    "### Multihead attention\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)\n",
    "- https://www.ibm.com/think/topics/attention-mechanism"
   ],
   "id": "1d3bdb424e13a234"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Bi-Encoder (e.g., BERT, RoBERTa)\n",
    "\n",
    "Bi-encoders independently encode two inputs (like a query and a document) into fixed-dimensional dense vectors using the same transformer model or encoder (with shared weights).\n",
    "Then it compares those embeddings - usually with cosine similarity or dot product. This type is often used for semantic search, information retrieval or re-ranking.\n",
    "\n",
    "**Pros**\n",
    "- Very fast at inference (because you can precompute document vectors).\n",
    "- Scales well for large retrieval (e.g., millions of docs).\n",
    "\n",
    "**Cons**\n",
    "- Because the texts are encoded separately, it can’t model deep interactions between words in query and document.\n",
    "- Less accurate than cross-encoders for fine-grained tasks.\n",
    "\n",
    "### Cross-Encoder\n",
    "\n",
    "Cross-encoders concatenate the two inputs into a single sequence and process them jointly in one pass through a transformer. This allows the model to attend to all tokens across both inputs simultaneously, capturing fine-grained interactions between them.\n",
    "\n",
    "**Pros**\n",
    "- Very accurate - because it captures cross-token attention between query and document.\n",
    "- Great for fine-grained tasks like ranking or classification.\n",
    "\n",
    "**Cons**\n",
    "- Slow and expensive - you must recompute the score for each pair.\n",
    "- Not scalable to millions of documents.\n",
    "\n",
    "### Decoder (e.g., GPT, Claude)\n",
    "\n",
    "Decoders are typically part of encoder-decoder architectures used in tasks like text generation. While bi- and cross-encoders focus on encoding inputs into vectors for similarity or classification, decoders generate output sequences based on the encoded input representations. So, decoders help transform encoded vectors back into meaningful text or other formats, which is particularly useful for generative models.\n",
    "\n",
    "### Encoder-Decoder (e.g., T5, BART)\n",
    "\n",
    "An encoder-decoder first encodes an input sequence into a fixed-length vector representation, which is then passed to a decoder to generate an output sequence. This architecture is commonly used for tasks like machine translation, text summarization, and image captioning."
   ],
   "id": "ecd0da45496a2206"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Basic usage of an embedding model\n",
    "\n",
    "#### Semantic Textual Similarity (STS)\n",
    "\n",
    "Produce embeddings for all texts involved and calculate the cosine-similarity between them. The text pairs with the highest similarity score are most semantically similar."
   ],
   "id": "77d1d6f2dc017183"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T18:09:42.817713Z",
     "start_time": "2025-11-18T18:09:40.297536Z"
    }
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)  # [3, 384]\n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "# compare each sentence A with each sentence B\n",
    "# 1 with 1,   1 with 2,   1 with 3\n",
    "# 2 with 1,   2 with 2,   ...\n",
    "# ...\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 20:09:40,321 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-11-18 20:09:40,322 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "74d7ab2f498640ecb9df2cbb0c3ac5b8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 384)\n",
      "tensor([[1.0000, 0.6660, 0.1046],\n",
      "        [0.6660, 1.0000, 0.1411],\n",
      "        [0.1046, 0.1411, 1.0000]])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Manual implementation (top 5 matches)\n",
    "\n",
    "For small corpora (up to about 1 million entries), we can perform semantic search with a manual implementation by computing the embeddings for the corpus with `SentenceTransformer.encode_document` as well as for our query with `SentenceTransformer.encode_query`, and then calculating the semantic textual similarity using `SentenceTransformer.similarity`.\n",
    "\n",
    "https://sbert.net/examples/sentence_transformer/applications/semantic-search/README.html"
   ],
   "id": "ae132b2cb917ffcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T18:09:44.846399Z",
     "start_time": "2025-11-18T18:09:42.836480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Corpus with example documents\n",
    "corpus = [\n",
    "    \"Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed.\",\n",
    "    \"Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning.\",\n",
    "    \"Neural networks are computing systems vaguely inspired by the biological neural networks that constitute animal brains.\",\n",
    "    \"Mars rovers are robotic vehicles designed to travel on the surface of Mars to collect data and perform experiments.\",\n",
    "    \"The James Webb Space Telescope is the largest optical telescope in space, designed to conduct infrared astronomy.\",\n",
    "    \"SpaceX's Starship is designed to be a fully reusable transportation system capable of carrying humans to Mars and beyond.\",\n",
    "    \"Global warming is the long-term heating of Earth's climate system observed since the pre-industrial period due to human activities.\",\n",
    "    \"Renewable energy sources include solar, wind, hydro, and geothermal power that naturally replenish over time.\",\n",
    "    \"Carbon capture technologies aim to collect CO2 emissions before they enter the atmosphere and store them underground.\",\n",
    "]\n",
    "# Use \"convert_to_tensor=True\" to keep the tensors on GPU (if available)\n",
    "corpus_embeddings = embedder.encode_document(corpus, convert_to_tensor=True)\n",
    "\n",
    "# Query sentences:\n",
    "queries = [\n",
    "    \"How do artificial neural networks work?\",\n",
    "    \"What technology is used for modern space exploration?\",\n",
    "    \"How can we address climate change challenges?\",\n",
    "]\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = min(5, len(corpus))\n",
    "for query in queries:\n",
    "    query_embedding = embedder.encode_query(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    similarity_scores = embedder.similarity(query_embedding, corpus_embeddings)[0]\n",
    "    scores, indices = torch.topk(similarity_scores, k=top_k)\n",
    "\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"Top 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for score, idx in zip(scores, indices):\n",
    "        print(f\"(Score: {score:.4f})\", corpus[idx])"
   ],
   "id": "53ad00b6a9637db3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 20:09:42,843 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-11-18 20:09:42,843 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c6e331695314c038858dac0b625693e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e518642142f947a4a93983fb2341268a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How do artificial neural networks work?\n",
      "Top 5 most similar sentences in corpus:\n",
      "(Score: 0.5926) Neural networks are computing systems vaguely inspired by the biological neural networks that constitute animal brains.\n",
      "(Score: 0.5288) Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning.\n",
      "(Score: 0.4647) Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed.\n",
      "(Score: 0.1381) Mars rovers are robotic vehicles designed to travel on the surface of Mars to collect data and perform experiments.\n",
      "(Score: 0.0912) Carbon capture technologies aim to collect CO2 emissions before they enter the atmosphere and store them underground.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cccd14973a94bbeacc41881fcd6aabd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What technology is used for modern space exploration?\n",
      "Top 5 most similar sentences in corpus:\n",
      "(Score: 0.3754) Mars rovers are robotic vehicles designed to travel on the surface of Mars to collect data and perform experiments.\n",
      "(Score: 0.3669) SpaceX's Starship is designed to be a fully reusable transportation system capable of carrying humans to Mars and beyond.\n",
      "(Score: 0.3452) The James Webb Space Telescope is the largest optical telescope in space, designed to conduct infrared astronomy.\n",
      "(Score: 0.2625) Renewable energy sources include solar, wind, hydro, and geothermal power that naturally replenish over time.\n",
      "(Score: 0.2275) Carbon capture technologies aim to collect CO2 emissions before they enter the atmosphere and store them underground.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e54b2b829614741a442491799b68e6d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: How can we address climate change challenges?\n",
      "Top 5 most similar sentences in corpus:\n",
      "(Score: 0.3760) Global warming is the long-term heating of Earth's climate system observed since the pre-industrial period due to human activities.\n",
      "(Score: 0.3144) Carbon capture technologies aim to collect CO2 emissions before they enter the atmosphere and store them underground.\n",
      "(Score: 0.2948) Renewable energy sources include solar, wind, hydro, and geothermal power that naturally replenish over time.\n",
      "(Score: 0.0420) Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed.\n",
      "(Score: 0.0411) Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Optimized implementation (`util.semantic_search`)",
   "id": "3cfaf11f7ff1f5f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T18:09:46.885446Z",
     "start_time": "2025-11-18T18:09:44.944022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sentence_transformers.util as util\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "corpus = [\n",
    "    \"Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed.\",\n",
    "    \"Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning.\",\n",
    "    \"Neural networks are computing systems vaguely inspired by the biological neural networks that constitute animal brains.\",\n",
    "    \"Mars rovers are robotic vehicles designed to travel on the surface of Mars to collect data and perform experiments.\",\n",
    "    \"The James Webb Space Telescope is the largest optical telescope in space, designed to conduct infrared astronomy.\",\n",
    "    \"SpaceX's Starship is designed to be a fully reusable transportation system capable of carrying humans to Mars and beyond.\",\n",
    "    \"Global warming is the long-term heating of Earth's climate system observed since the pre-industrial period due to human activities.\",\n",
    "    \"Renewable energy sources include solar, wind, hydro, and geothermal power that naturally replenish over time.\",\n",
    "    \"Carbon capture technologies aim to collect CO2 emissions before they enter the atmosphere and store them underground.\",\n",
    "]\n",
    "corpus_embeddings = embedder.encode_document(corpus, convert_to_tensor=True)\n",
    "\n",
    "queries = [\n",
    "    \"How do artificial neural networks work?\",\n",
    "    \"What technology is used for modern space exploration?\",\n",
    "    \"How can we address climate change challenges?\",\n",
    "]\n",
    "\n",
    "top_k = min(5, len(corpus))\n",
    "\n",
    "query_embeddings = embedder.encode_query(queries, convert_to_tensor=True)\n",
    "results = util.semantic_search(query_embeddings, corpus_embeddings, top_k=top_k)\n",
    "\n",
    "print(f\"Top {top_k} most similar sentences in corpus for each query:\")\n",
    "for q, qr in zip(queries, results):\n",
    "    print(\"\\nQuery:\", q)\n",
    "\n",
    "    for r in qr:\n",
    "        score = r[\"score\"]\n",
    "        idx = r[\"corpus_id\"]\n",
    "        print(f\"(Score: {score:.4f})\", corpus[idx])"
   ],
   "id": "74edf2083624b0fc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 20:09:44,949 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-11-18 20:09:44,950 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a6f9cb99b174490a44190d370bb3db3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d63f0db78154b72a3953d2c9a73dc10"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar sentences in corpus for each query:\n",
      "\n",
      "Query: How do artificial neural networks work?\n",
      "(Score: 0.5926) Neural networks are computing systems vaguely inspired by the biological neural networks that constitute animal brains.\n",
      "(Score: 0.5288) Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning.\n",
      "(Score: 0.4647) Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed.\n",
      "(Score: 0.1381) Mars rovers are robotic vehicles designed to travel on the surface of Mars to collect data and perform experiments.\n",
      "(Score: 0.0912) Carbon capture technologies aim to collect CO2 emissions before they enter the atmosphere and store them underground.\n",
      "\n",
      "Query: What technology is used for modern space exploration?\n",
      "(Score: 0.3754) Mars rovers are robotic vehicles designed to travel on the surface of Mars to collect data and perform experiments.\n",
      "(Score: 0.3669) SpaceX's Starship is designed to be a fully reusable transportation system capable of carrying humans to Mars and beyond.\n",
      "(Score: 0.3452) The James Webb Space Telescope is the largest optical telescope in space, designed to conduct infrared astronomy.\n",
      "(Score: 0.2625) Renewable energy sources include solar, wind, hydro, and geothermal power that naturally replenish over time.\n",
      "(Score: 0.2275) Carbon capture technologies aim to collect CO2 emissions before they enter the atmosphere and store them underground.\n",
      "\n",
      "Query: How can we address climate change challenges?\n",
      "(Score: 0.3760) Global warming is the long-term heating of Earth's climate system observed since the pre-industrial period due to human activities.\n",
      "(Score: 0.3144) Carbon capture technologies aim to collect CO2 emissions before they enter the atmosphere and store them underground.\n",
      "(Score: 0.2948) Renewable energy sources include solar, wind, hydro, and geothermal power that naturally replenish over time.\n",
      "(Score: 0.0420) Machine learning is a field of study that gives computers the ability to learn without being explicitly programmed.\n",
      "(Score: 0.0411) Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Semantic search over PDF\n",
    "\n",
    "- https://github.com/explosion/spacy-layout\n",
    "- https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ],
   "id": "bd095ec64e24493"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T18:09:46.965328Z",
     "start_time": "2025-11-18T18:09:46.962452Z"
    }
   },
   "cell_type": "code",
   "source": "# %python -m spacy download en_core_web_trf",
   "id": "e4e4d76b6508cbb7",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T18:10:08.753945Z",
     "start_time": "2025-11-18T18:09:46.971634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy_layout import spaCyLayout\n",
    "\n",
    "# Read PDF\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "layout = spaCyLayout(nlp)\n",
    "\n",
    "# Process a document and create a spaCy Doc object\n",
    "# There is also a possibility to process several files with `layout.pipe(paths)`\n",
    "doc = layout(\"../semantic_search/Attention Is All You Need.pdf\")\n",
    "\n",
    "# Filter spans for body text\n",
    "text_spans = [\n",
    "    span.text.strip() for span in doc.spans.get(\"layout\", [])\n",
    "    if span.label_ == \"text\"\n",
    "]\n",
    "\n",
    "text = \" \".join(text_spans)\n",
    "\n",
    "doc = nlp(text)\n",
    "sentences = [s.text.strip() for s in doc.sents if s.text.strip()]\n",
    "print(\"Total sentences:\", len(sentences))\n",
    "\n",
    "# Embed the sentences\n",
    "pdf_embedder = SentenceTransformer(\"all-MiniLM-L12-v2\", device=\"cuda\")\n",
    "\n",
    "pdf_embeddings = pdf_embedder.encode_document(sentences, convert_to_tensor=True)\n",
    "pdf_embeddings = util.normalize_embeddings(pdf_embeddings)"
   ],
   "id": "9c0c503c341babd8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 20:09:48,492 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-18 20:09:48,498 - INFO - Going to convert document batch...\n",
      "2025-11-18 20:09:48,499 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 44ae89a68fc272bc7889292e9b5a1bad\n",
      "2025-11-18 20:09:48,502 - INFO - rapidocr cannot be used because onnxruntime is not installed.\n",
      "2025-11-18 20:09:48,503 - INFO - easyocr cannot be used because it is not installed.\n",
      "2025-11-18 20:09:48,504 - INFO - Accelerator device: 'cuda:0'\n",
      "\u001B[32m[INFO] 2025-11-18 20:09:48,521 [RapidOCR] base.py:22: Using engine_name: torch\u001B[0m\n",
      "\u001B[32m[INFO] 2025-11-18 20:09:48,532 [RapidOCR] download_file.py:60: File exists and is valid: E:\\Work\\TNTU\\PhD\\subjects\\AI\\AI-fundamentals\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001B[0m\n",
      "\u001B[32m[INFO] 2025-11-18 20:09:48,533 [RapidOCR] torch.py:54: Using E:\\Work\\TNTU\\PhD\\subjects\\AI\\AI-fundamentals\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.pth\u001B[0m\n",
      "\u001B[32m[INFO] 2025-11-18 20:09:48,659 [RapidOCR] base.py:22: Using engine_name: torch\u001B[0m\n",
      "\u001B[32m[INFO] 2025-11-18 20:09:48,661 [RapidOCR] download_file.py:60: File exists and is valid: E:\\Work\\TNTU\\PhD\\subjects\\AI\\AI-fundamentals\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001B[0m\n",
      "\u001B[32m[INFO] 2025-11-18 20:09:48,662 [RapidOCR] torch.py:54: Using E:\\Work\\TNTU\\PhD\\subjects\\AI\\AI-fundamentals\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_ptocr_mobile_v2.0_cls_infer.pth\u001B[0m\n",
      "\u001B[32m[INFO] 2025-11-18 20:09:48,712 [RapidOCR] base.py:22: Using engine_name: torch\u001B[0m\n",
      "\u001B[32m[INFO] 2025-11-18 20:09:48,730 [RapidOCR] download_file.py:60: File exists and is valid: E:\\Work\\TNTU\\PhD\\subjects\\AI\\AI-fundamentals\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001B[0m\n",
      "\u001B[32m[INFO] 2025-11-18 20:09:48,731 [RapidOCR] torch.py:54: Using E:\\Work\\TNTU\\PhD\\subjects\\AI\\AI-fundamentals\\.venv\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.pth\u001B[0m\n",
      "2025-11-18 20:09:49,347 - INFO - Auto OCR model selected rapidocr with torch.\n",
      "2025-11-18 20:09:49,348 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-11-18 20:09:50,264 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-11-18 20:09:50,655 - INFO - Processing document Attention Is All You Need.pdf\n",
      "2025-11-18 20:10:02,743 - INFO - Finished converting document Attention Is All You Need.pdf in 14.25 sec.\n",
      "2025-11-18 20:10:02,784 - WARNING - Usage of TableItem.export_to_dataframe() without `doc` argument is deprecated.\n",
      "2025-11-18 20:10:02,785 - WARNING - Usage of TableItem.export_to_dataframe() without `doc` argument is deprecated.\n",
      "2025-11-18 20:10:02,787 - WARNING - Usage of TableItem.export_to_dataframe() without `doc` argument is deprecated.\n",
      "2025-11-18 20:10:02,790 - WARNING - Usage of TableItem.export_to_dataframe() without `doc` argument is deprecated.\n",
      "2025-11-18 20:10:05,777 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29fa82527af744eba3f86ef5d6746f8a"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T18:10:08.879528Z",
     "start_time": "2025-11-18T18:10:08.856571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Semantic search\n",
    "queries = [\"accuracy\"]\n",
    "\n",
    "top_k = min(5, len(corpus))\n",
    "\n",
    "pdf_query_embeddings = pdf_embedder.encode_query(queries, convert_to_tensor=True)\n",
    "\n",
    "pdf_query_embeddings = pdf_query_embeddings.to(\"cuda\")\n",
    "pdf_query_embeddings = util.normalize_embeddings(pdf_query_embeddings)\n",
    "\n",
    "results = util.semantic_search(pdf_query_embeddings, pdf_embeddings, top_k=top_k)\n",
    "\n",
    "print(f\"Top {top_k} most similar sentences in corpus for each query:\")\n",
    "for q, qr in zip(queries, results):\n",
    "    print(\"\\nQuery:\", q)\n",
    "\n",
    "    for r in qr:\n",
    "        score = r[\"score\"]\n",
    "        idx = r[\"corpus_id\"]\n",
    "        print(f\"(Score: {score:.4f})\", sentences[idx])"
   ],
   "id": "abe0adba3ee5b07a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fb8e672332c14c06bbae375712bd13f2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most similar sentences in corpus for each query:\n",
      "\n",
      "Query: accuracy\n",
      "(Score: 0.4139) This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n",
      "(Score: 0.3691) We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5 .\n",
      "(Score: 0.3497) During training, we employed label smoothing of value ϵ ls = 0 . 1 [36].\n",
      "(Score: 0.3126) Acknowledgements\n",
      "(Score: 0.3122) We trained a 4-layer transformer with d model = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences.\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
