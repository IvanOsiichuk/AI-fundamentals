{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Back propagation\n",
    "\n",
    "## Definition\n",
    "\n",
    "In machine learning, backpropagation is a gradient computation method commonly used for training a neural network in computing parameter updates based on the chain rule. Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input–output example, calculating the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Backpropagation"
   ],
   "id": "57a45ab79297d606"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initial parameters",
   "id": "f4cae82d26a2d01c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.231203Z",
     "start_time": "2025-10-11T18:10:51.228362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = np.array([1, 0])\n",
    "y_expected = 0\n",
    "\n",
    "weights_hidden = np.array([\n",
    "    [0.2, 0.4, 0.7, 0.5],\n",
    "    [0.3, 0.5, 0.6, 0.9]\n",
    "])\n",
    "weights_output = np.array(\n",
    "    [0.2, 0.4, 0.6, 0.8]\n",
    ")\n",
    "\n",
    "bias_hidden = 1\n",
    "bias_output = 1\n",
    "\n",
    "n_learning = 0.8"
   ],
   "id": "b5063ebdad87560a",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FORWARD FEED",
   "id": "8213d926b426b317"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hidden layer pre-activation\n",
    "\n",
    "Weighted sum of inputs for each hidden neuron:\n",
    "\n",
    "$$\n",
    "Z_{hidden} = \\sum_{i=1}^{N} x_i w_{hidden} + b_{hidden} \\tag{1}\n",
    "$$"
   ],
   "id": "fecd5f1fb303cc05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.241417Z",
     "start_time": "2025-10-11T18:10:51.238816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Z_hidden = np.dot(inputs, weights_hidden) + bias_hidden\n",
    "print(Z_hidden)"
   ],
   "id": "bd91fde9e4d5d33a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2 1.4 1.7 1.5]\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hidden layer activation (sigmoid)\n",
    "\n",
    "$$\n",
    "y_{\\text{hidden}} = \\frac{1}{1 + e^{-Z_{\\text{hidden}}}} \\tag{2}\n",
    "$$"
   ],
   "id": "ca30eb8962d22ac6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.257919Z",
     "start_time": "2025-10-11T18:10:51.255165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "y_hidden = sigmoid(Z_hidden)\n",
    "print(y_hidden)"
   ],
   "id": "bc31f7832a3fc7cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76852478 0.80218389 0.84553473 0.81757448]\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Output layer pre-activation\n",
    "\n",
    "$$\n",
    "Z_{output} = \\sum_{i=1}^{N} y_{hidden_i} w_{output_i} + b_{output} \\tag{3}\n",
    "$$"
   ],
   "id": "a1340ea0fbabe723"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.271117Z",
     "start_time": "2025-10-11T18:10:51.268480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Z_output = np.sum(y_hidden * weights_output) + bias_output  # one output neuron\n",
    "print(Z_output)"
   ],
   "id": "b94b1089b26bc371",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6359589340280305\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Output layer activation\n",
    "\n",
    "$$\n",
    "y_{\\text{output}} = \\frac{1}{1 + e^{-Z_{\\text{output}}}} \\tag{4}\n",
    "$$"
   ],
   "id": "eaab7691a06fd8c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.283110Z",
     "start_time": "2025-10-11T18:10:51.280341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_output = sigmoid(Z_output)\n",
    "print(y_output)"
   ],
   "id": "7163261bbbe671d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9331402852352827\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loss function calculation\n",
    "\n",
    "Square error:\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2}(y_{\\text{expected}} - y_{\\text{output}})^2 \\tag{5}\n",
    "$$"
   ],
   "id": "865e3b659a54c9b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.294668Z",
     "start_time": "2025-10-11T18:10:51.292031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "E = pow(y_expected - y_output, 2) / 2\n",
    "print(E)"
   ],
   "id": "238eaea92ceab2c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4353753959644924\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## OUTPUT LAYER ADJUSTMENT",
   "id": "ef51e43713cf0a3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Gradient of the loss function with respect to the hidden-output weights\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{output_i}} = \\frac{\\partial E}{\\partial y_{\\text{output}}} \\frac{\\partial y_{\\text{output}}}{\\partial z_{\\text{output}}} \\frac{\\partial z_{\\text{output}}}{\\partial w_{output_i}} \\tag {6}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial y_{\\text{output}}} = -(y_{\\text{очік}} - y_{\\text{output}}) \\tag{7}$$\n",
    "$$\\frac{\\partial y_{\\text{output}}}{\\partial z_{\\text{output}}} = y_{\\text{output}}(1 - y_{\\text{output}}) \\tag{8}$$\n",
    "$$\\frac{\\partial z_{\\text{output}}}{\\partial w_k} = y_{\\text{hidden}_i} \\tag{9}$$"
   ],
   "id": "51e9b2e428373fa8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.307156Z",
     "start_time": "2025-10-11T18:10:51.304271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dE_to_dY_output = -(y_expected - y_output)\n",
    "dY_output_to_dZ_output = y_output * (1 - y_output)\n",
    "dZ_output_to_dW_output = y_hidden\n",
    "\n",
    "weights_output_gradient = dE_to_dY_output * dY_output_to_dZ_output * dZ_output_to_dW_output\n",
    "print(weights_output_gradient)"
   ],
   "id": "79d5ba9fce2ef086",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04474209 0.04670166 0.04922547 0.04759767]\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Gradient of the loss function with respect to the output bias\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial b_{output}} = \\frac{\\partial E}{\\partial y_{\\text{output}}} \\frac{\\partial y_{\\text{output}}}{\\partial z_{\\text{output}}} \\frac{\\partial z_{\\text{output}}}{\\partial b_{output}} \\tag{10}\n",
    "$$\n",
    "\n",
    "The first two derivatives of the previous equation are defined as **(7)** and **(8)**.\n",
    "\n",
    "Due to the linear dependence of Z on b (Z directly increases by b):\n",
    "\n",
    "$$\\frac{\\partial z_{\\text{output}}}{\\partial b_{output}} = 1 \\tag{11}$$"
   ],
   "id": "1566347ea2f7507b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.322174Z",
     "start_time": "2025-10-11T18:10:51.318978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dZ_output_to_dB_output = 1 # TODO: check\n",
    "# dB does not impact result - added only for clarity\n",
    "bias_output_gradient = dE_to_dY_output * dY_output_to_dZ_output * dZ_output_to_dB_output\n",
    "print(bias_output_gradient)"
   ],
   "id": "d095e6d33286492c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05821814957952363\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hidden-output weights adjustment\n",
    "\n",
    "$$\\widehat{w}_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i} \\tag{12}$$"
   ],
   "id": "a38533b81ec97d83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.349404Z",
     "start_time": "2025-10-11T18:10:51.346621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights_output_adjusted = weights_output - n_learning * weights_output_gradient\n",
    "print(weights_output_adjusted)"
   ],
   "id": "1937c0aff35aa5f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16420633 0.36263867 0.56061963 0.76192186]\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Output bias adjustment\n",
    "\n",
    "$$\\widehat{b} = b - \\eta \\frac{\\partial E}{\\partial b} \\tag{13}$$"
   ],
   "id": "ab7f49996708af38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.418747Z",
     "start_time": "2025-10-11T18:10:51.416315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bias_output_adjusted = bias_output - n_learning * bias_output_gradient\n",
    "print(bias_output_adjusted)"
   ],
   "id": "ee6b9a79325841dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9534254803363811\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HIDDEN LAYER ADJUSTMENT",
   "id": "55d82cb27309bac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Gradient of the loss function with respect to the input-hidden weights\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{hidden_i}} = \\frac{\\partial E}{\\partial y_{hidden_i}} \\frac{\\partial y_{hidden_i}}{\\partial z_{hidden_i}} \\frac{\\partial z_{hidden_i}}{\\partial w_{hidden_k}} \\tag{14}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial y_{hidden_i}} = \\frac{\\partial E}{\\partial y_{\\text{output}}} \\cdot \\frac{\\partial y_{\\text{output}}}{\\partial z_{\\text{output}}} \\cdot \\frac{\\partial z_{\\text{output}}}{\\partial y_{hidden_i}}    ;   → \\frac{\\partial z_{\\text{output}}}{\\partial y_{\\text{hidden}_i}} = w_{{output_i}} \\tag{15; 16}$$\n",
    "The first two derivatives of the previous equation are defined as **(7)** and **(8)**.\n",
    "\n",
    "$$\\frac{\\partial y_{hidden_i}}{\\partial z_{hidden_i}} = y_{hidden_i} (1 - y_{hidden_i}) \\tag{17}$$\n",
    "\n",
    "$$\\frac{\\partial z_{hidden_i}}{\\partial w_{hidden_k}} = x_i \\tag{18}$$"
   ],
   "id": "740e91447c1b2eda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.531617Z",
     "start_time": "2025-10-11T18:10:51.528398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: dZ_output/dY_hidden = weights_input vs weights_output?\n",
    "# dZ_output_to_dY_hidden = weights_input  # (2, 4)\n",
    "dZ_output_to_dY_hidden = weights_output  # (4, 1)\n",
    "\n",
    "dE_to_dY_hidden = dE_to_dY_output * dY_output_to_dZ_output * dZ_output_to_dY_hidden\n",
    "dY_hidden_to_dZ_hidden = y_hidden * (1 - y_hidden)  # (4, 1)\n",
    "dZ_hidden_to_dW_hidden = inputs  # (2, 1)\n",
    "\n",
    "# dE/dw_hidden = dE/dY_hidden * dY_hidden/dZ_hidden * dZ_hidden/dW_hidden\n",
    "# inputs have different shape than other components, so we need to multiply each input by each element of (4, 1)\n",
    "# np.outer(A, B) - multiply each element of A by each element of B (element-wise)\n",
    "weights_hidden_gradient = np.outer(dZ_hidden_to_dW_hidden, dE_to_dY_hidden * dY_hidden_to_dZ_hidden)\n",
    "print(weights_hidden_gradient)"
   ],
   "id": "e266d59b4f8c8906",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00207134 0.00369534 0.00456217 0.00694642]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Gradient of the loss function with respect to the hidden bias\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial b_{hidden}} = \\frac{\\partial E}{\\partial y_{hidden_i}} \\frac{\\partial y_{hidden_i}}{\\partial z_{hidden_i}} \\frac{\\partial z_{hidden_i}}{\\partial b_{hidden}} \\tag{19}$$\n",
    "\n",
    "Due to the linear dependence of Z on b (Z directly increases by b):\n",
    "\n",
    "$$\\frac{\\partial z_{hidden_i}}{\\partial b_{hidden}} = 1 \\tag{20}$$"
   ],
   "id": "f239f1a036097891"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.558132Z",
     "start_time": "2025-10-11T18:10:51.555194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dZ_hidden_to_dB_hidden = 1 # TODO: check\n",
    "\n",
    "# dB does not impact result - added only for clarity\n",
    "bias_hidden_gradient = dE_to_dY_hidden * dY_hidden_to_dZ_hidden * dZ_hidden_to_dB_hidden\n",
    "print(bias_hidden_gradient)"
   ],
   "id": "39e39e59a29ce6e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00207134 0.00369534 0.00456217 0.00694642]\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Input-hidden weights adjustment\n",
    "\n",
    "$$\\widehat{w}_i = w_i - \\eta \\frac{\\partial E}{\\partial w_i}$$\n",
    "\n",
    "The same formula as **(12)**."
   ],
   "id": "963674e8f794aab1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.575164Z",
     "start_time": "2025-10-11T18:10:51.572028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights_hidden_adjusted = weights_hidden - n_learning * weights_hidden_gradient\n",
    "print(weights_hidden_adjusted)"
   ],
   "id": "9ff1e180b6381cce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19834293 0.39704373 0.69635026 0.49444286]\n",
      " [0.3        0.5        0.6        0.9       ]]\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hidden bias adjustment\n",
    "\n",
    "$$\\widehat{b} = b - \\eta \\frac{\\partial E}{\\partial b}$$\n",
    "\n",
    "The same formula as **(13)**.\n",
    "\n",
    "Hidden bias gradient has individual value per neuron (4 values in this case), so we use cumulative gradient by adding all gradients."
   ],
   "id": "3cfe95dc198662c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-11T18:10:51.596246Z",
     "start_time": "2025-10-11T18:10:51.593042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bias_hidden_cumulative_gradient = bias_hidden_gradient.sum() # TODO: check if sum is right here\n",
    "bias_hidden_adjusted = bias_hidden - n_learning * bias_hidden_cumulative_gradient\n",
    "print(bias_hidden_adjusted)"
   ],
   "id": "33200dfa9f3f5064",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9861797817737953\n"
     ]
    }
   ],
   "execution_count": 63
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
